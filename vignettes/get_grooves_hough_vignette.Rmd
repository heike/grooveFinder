---
title: "Groove Identification Using Hough Transforms"
author: "Charlotte Roiger"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


Hough transforms are a low-level computer vision algorithm used to detect basic shapes in an image array. The earliest case was for detecting a line in an image, but since then, has expanded to include circles and other such shapes. For bullet processing, the borders of a groove engraved area are often deep linear divets in a bullet land, so Hough transforms are useful for distinguishing between land engraved areas(LEAs) and groove engraved area(GEAs). We will cover the process of using Hough transforms to find GEAs from bullet pre-processing to method comparison. 

# Bullet Pre-Processing

We will use demo data available in the `grooveFinder` package to demonstrate how to find grooves using Hough transforms. It's important to note, that unlike other methods available in `grooveFinder`, `get_grooves_hough` requires the entirety of the x3p for analysis rather than a single or averaged crosscut. See package `x3ptools` at https://heike.github.io/x3ptools/ for ways to convert different file formats into x3p standard files. Note that the usage of the `imager` package requires the installation of `XQuartz` and `ImageMagick`.

```{r warning = F, message= F}

# Load in Libraries

library(ggplot2) # Utilize to visualize cross cuts and bullet lands
library(dplyr) # Used to transform and cleaning up data
library(x3ptools) # Contains a host of useful functions for dealing with x3ps
library(imager) # Used for converting x3ps to cimgs and running the Hough transform
library(bulletxtrctr) 
library(grooveFinder) 
#library(fixedpoints) this package is used to run Canny Edge detection
```

The data we will be working with is from the Hamby 44 dataset from the NIST Research Ballistics Toolmarks data base (NRBTD)[https://tsapps.nist.gov/NRBTD/Studies/Search] These are commands loading the bullet scans into `R` without downloading the actual files.


```{r}
# Load in data using weblinks
b1 <- read_bullet(urllist = hamby44demo[[1]])
b2 <- read_bullet(urllist = hamby44demo[[2]])
```

For ease of use we will combine the data into one dataframe.

```{r}
b1$bullet <- 1
b2$bullet <- 2
b1$land <- 1:6
b2$land <- 1:6
bullets <- rbind(b1, b2)
```

The measurement of each scan should be in microns, however checking the units as shown below indicates that it is recorded in meters. 

```{r}

# Check units
bullets$x3p[[1]]$header.info$incrementY

# Convert to microns
bullets <- bullets %>% mutate(
  x3p = x3p %>% purrr::map(.f = x3p_m_to_mum)
)

# Check units once more
bullets$x3p[[1]]$header.info$incrementY

```

Now that the land is in microns we can view the bullet land using the `image_x3p` functions from `x3ptools`. Notice how the very bottom part of the land has an area that looks fairly smoothe.
We expect this smoothe area to be at the bottom of the land, so we need to flip
the image over the x-axis. 


```{r, eval=FALSE}
image_x3p(bullets$x3p[[1]], file = "../man/figures/temp-before.png")
```
```{r, echo = FALSE}
knitr::include_graphics("../man/figures/temp-before.png")
```



```{r}
# flip scan so that the foot is at the bottom

bullets <- bullets %>% mutate(
  x3p = x3p %>% purrr::map(.f = function(x) x %>% 
                             y_flip_x3p())
)   
```

```{r, echo = F}
image_x3p(bullets$x3p[[1]], file = "../man/figures/after-before.png")
```
```{r, echo = F}
knitr::include_graphics("../man/figures/after-before.png")
```

Not all sets of bullet land data may be flipped in the same way as the data presented in this vignette, so viewing the image and image units should be carried out everytime before analysis.

# Application of the Hough Transform

There are essentially three steps wrapped into the `get_grooves_hough` function that are needed to identify GEAs using Hough transforms: creating an image gradient, applying the Hough transform, and selecting the best Hough estimate for the grooves. While all three of these steps are taken care of automatically within the function, a discussion of the full mechanics of the algorithm and the parametrization of the results is necessary for better understanding.

## Image Gradient

Traditionally when using the Hough transform a couple of different image cleaning algorithms are employed to highlight geometric features within the image. Typically images are first run through a Gaussian filter then a process known as Canny Edge detection is employed. The goal of these two processes is to eliminate extraneous image colors or features and to emphasize prominent lines. We have found that neither the Gaussian filter nor the Canny Edge detection process is necessary to identify bullet grooves and in fact increases processing time. To develop these processes we made great use of the [vignettes](https://dahtah.github.io/imager/canny.html) from the `Imager` package by Simon Barthelm√©.


By electing to use the Hough transform we are choosing to reduce the dimensionality of our data. The x3p presents valuable three dimensional data that we will have to reduce to two dimensions in order to perform image analysis. The image format of choice for the `imager` package is called a c-image, shown below.

```{r}
land <- as.cimg(bullets$x3p[[1]]$surface.matrix)
```

```{r echo = FALSE, eval = FALSE}
raster <- imager:::as.data.frame.cimg(land)
ggplot() +
  geom_raster(data = raster, aes(x = x, y = -y, fill = value)) +
  coord_fixed() +
  scale_fill_gradientn(colours=c("#000000","#FFFFFF"))+ 
  guides(fill = FALSE)
ggsave(filename = "../man/figures/cimg-land.png", width = 4, height= 3, units = "in", dpi = 175)

```

```{r, echo = F}
knitr::include_graphics("../man/figures/cimg-land.png")

```

Now that we have a two dimensional visualization of our bullet land we can create an image gradient, which detects changes in direction, intensity, or color of pixels in our c-image. Then we filter our gradient image by only selecting to pixel gradient values in the top 99.9th percentage. This eliminates some extraneous details from our image in preparation for either the Hough transform or the Canny Edge Detection.

```{r}
# Create gradient image from the c-image
dx <- imgradient(land, "x")
dy <- imgradient(land, "y")

grad.mag <- sqrt(dx^2 + dy^2)

# Filter gradient image so that only the strongest gradients are highlighted
strong <- grad.mag > quantile(grad.mag, .99, na.rm = TRUE )
```

To carry out Canny Edge detection further processing is needed as outlined in the `Imager` Canny Edge [vignette](https://dahtah.github.io/imager/canny.html). 

```{r eval = F}

# Select only the weaker edges in the c-image
t2 <- quantile(grad.mag, .99, na.rm = TRUE)
t1 <- quantile(grad.mag, .8, na.rm = TRUE)

weak<- grad.mag %inr% c(t1,t2)


# Perform Canny Edge using hysteresis 

expand.strong <- function(ws){
  overlap <- grow(ws$strong, 3) & ws$weak
  ws$strong[overlap] <- TRUE
  ws$weak[overlap <- FALSE]
  ws 
}

# Requires fixed points package: devtools::install_github("dahtah/fixedpoints")
hystFP <- fixedpoints::fp(expand.strong)
system.time(out <- list(strong = strong, weak = weak) %>% hystFP)

```

So there appears to be an elapsed time of around 25-30 seconds, which isn't much but simply adds to the processing time taken to analyze each bullet land. Below we see two dimensional visualization of the Canny Edge detection and the gradient image threshold.

```{r echo = FALSE, eval = FALSE}

hyst.raster <- imager:::as.data.frame.cimg(out$strong)

ggplot() +
  geom_raster(data = hyst.raster, aes(x = x, y = -y, fill = value)) +
  scale_fill_manual(values = c("black", "white", "grey"))+
  coord_fixed() +
  labs(title = "Canny-edge Detection of Bullet Land")+
  guides(fill = FALSE)

ggsave(filename = "../man/figures/canny-land.png", width = 4, height= 3, units = "in", dpi = 175)

strong.raster <- imager:::as.data.frame.cimg(strong)
ggplot() +
  geom_raster(data = strong.raster, aes(x = x, y = -y, fill = value)) +
  scale_fill_manual(values = c("black", "white", "grey"))+
  coord_fixed() +
  labs(title = "Strength-thresholded Bullet Land") +
  guides(fill = FALSE)

ggsave(filename = "../man/figures/strong-threshold.png", width = 4, height= 3, units = "in", dpi = 175)

```

```{r, echo = F}
knitr::include_graphics("../man/figures/canny-land.png")
knitr::include_graphics("../man/figures/strong-threshold.png")

```


Notice how in the Canny-Edge detection image, the Canny edge picks up a couple strong striae. We do not want the Hough transform to mistake these strong striae for viable borders of the groove engraved areas and so the strength thresholding pre-processing method was selected to minimize this risk and slightly improve processing time.

## Applying Hough Transform

Once pre-processing has been completed, we can now implement the Hough transform process using the `hough_lines` function from `imager`. Because we are working in images that start at the origin we need to shift the hough transform results and return a dataframe for line selection processes. 

```{r}
# We want to get values with respect to (0,0) not (1,1)
 hough.df <- hough_line(strong, data.frame = TRUE, shift = FALSE) 
```

Before continuing on with the Hough transform process, a discussion of the parametrization and process is necessary to facilitate understanding. The actual Hough algorithm cycles through every pixel of the gradient image we've created labeled "strong". For every possible non-zero edge point, an equivalent sinusoidal curve is generated in the feature space. Consequently points on a line will generate sinusoidal curves that all intersect at a particular set of parameters that best describe the line detected. So not only does the Hough transform suggest parameters for edges detected it also yields a "score" which counts the number of lines that intersect at each point in the feature space, or roughtly how many pixels in our image fall on the detected lines. 

A note about the feature space, since the slope of vertical lines in the x,y-plane is infinite the amount of memory required to store results in the feature space would make this algorithm unfeasible. So to manage with this problem, the Hough transform characterizes line in Hessian Normal form, meaning each line is described by a $\rho$ and a $\theta$ where $\rho$ is an orthogonal vector from the origin to a point on a detected line, and $\theta$ is the angle between the orthogonal vector and the postivie x-axis. 

```{r, echo = F}
knitr::include_graphics("../man/figures/hessian-example.png", dpi = 300)
```


These results are useful but in order to use the Hough transform we need to decide how to transform the $\rho$ and $\theta$ parameters back into the x,y-plane. We note that a link between these two parametrizations of lines is given by the following formula for the x and y intercepts:

$$\rho = x_o\cos(\theta) + y_o\sin(\theta)$$

Where $x_o$ is the x intercept and $y_o$ is the y-intercept. When $\theta$ is zero, we can easily find the x-intercept of any Hough line by finding $\frac{\rho}{cos(\theta)}$. This is accomplished in a helper function called `rho_to_ab` which is not exported as part of the `groovFinder` package. We utilize this helper function to then find the points of each hough line where the Hough line intersects the top and bottom of each land. 

```{r eval = FALSE}

segments <-  segments <- grooveFinder:::rho_to_ab(df = hough.df)

```


```{r, echo = FALSE, eval = FALSE}
library(cowplot)
p <- ggplot() +
  geom_segment(aes(x = 174, y = 0, xend = 400, yend = - 100))

ggdraw() +
  draw_image("../man/figures/after-before.png", scale = 0.8) +
  draw_plot(p)
  
```

The top of the land, designated "xtop" in our code is simply the x-intercept calculated by `rho_to_ab`. The bottom of the land, designated "xbottom" is found geometrically. As shown in the figure below, we can draw a triangular shape between "xtop" the bottom of the bullet land and our supposed lower x-intercept for a Hough estimate. Given that we know the angle $\theta$ we can then find the location of "xbottom": $\text{xtop} - (\text{height of the land}) * tan(\theta)$ is the height of the land multiplied by tangent of the angle theta subtracted from "xtop. 

INSERT IMAGE HERE FOR CALCULATING XBOTTOM

We then find the slope in y of each Hough line by dividing the difference in x between "xtop" and "xbottom" by the height of the bullet land. We utilize the slope in y rather than the slope in x because i

```{r, eval = F}
segments <- segments %>%
    dplyr::mutate(
      xbottom = xintercept - height*tan(theta),
      xtop = xintercept,
      norm.score = score / (height/cos(theta)),
      slope_y = (xtop - xbottom)/height # Choosing to use slope in y because it is more robust
    ) %>%
    dplyr::arrange(desc(norm.score))
```

INSERT IMAGE HERE FOR CALCULATING THE NORMALIZED SCORE

As previously discussed, the Hough transform outputs a score that can be interpreted as the number of points detected for a particular line detected in the image. Theoretically the strongest line detected in the image should be the groove locations. However, score alone is not a good enough metric to choose groove locations, we need to first normalize the score of each Hough line by dividing the score by the largest possible number of points detected in the line. We take into consideration only lines that intersect both the top and bottom of the land, and find that the largest possible score for each Hough estimate would be: $\text{Height of the bullet land}/cos(\theta)$. So we divide each observed Hough score by the maximum score possible for lines at that angle. To select each groove we first select only Hough estimates below the lower one-sixth of the bullet land and grooves above the upper five-sixth of the land. 

```{r eval = FALSE}
 # Find the middle 2/3rds
  lthird <- width / 6
  uthird <- 5 * width / 6

  # separate into left and right sides and only take into account bullet lands that intersect with the image bottom

  segments.left <- segments %>%
    filter(rho < lthird,
           xbottom > 0)

  segments.right <- segments %>%
    filter(rho > uthird,
           xbottom < width)
  
  # select largest normalized score parametrization
  largest.norm.left <- segments.left[norm.index,]
  largest.norm.right <- segments.right[norm.index,]

```


These scores are then arranged in descending order and the line with the highest normalized Hough score is selected to by our estimated groove location. Once these particular lines are selected, we need to reformat our estimates into functions so they can be applied to particulr crosscuts or used to create a three-dimensional visualization
